# 模式识别实验（二）实验报告

## 学生信息

- 姓名-学号-班级
- 李欢欢-2020904302-2020240402
- 刘易行-2020905896-2020240401

## 实验题目

![image-20221027163916552](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221027163916552.png)

## 问题回答

### （1）算法体现

![image-20221027163946160](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027163946160.png)

```python
def ClassifySamples(sample,PCi,VEC_P):
    """
    Parameters:
    sample: 训练样本 10*784    类型:二维ndarray
    PCJ: 先验概率   10    类型:float64
    VEC_P: 即E矩阵
    """
    step1 = np.subtract(sample,1)  #数组数据相减
    step2 = np.abs(step1)  #求出绝对值
    step3 = np.subtract(step2,VEC_P)  #再相减
    step4=np.abs(step3) #求出最后的值
    log_P_Ci = np.log(PCi)
    log_P_Xj_Ci = np.log(step4)
    #计算P(Ci|x)=log(P(Ci))+Σlog(P(Xj|Ci))
    P_Ci_X = np.add(log_P_Ci,np.sum(log_P_Xj_Ci,axis=1))
    return np.argmax(P_Ci_X)
```

### （2）(a)分类器是如何设计的？

- 此次实验基于书本P41页的独立二值特征的贝叶斯分类问题

  #### 数据处理

- 先来看下数据集的格式mnist_train.csv  mnist_test.csv

- csv格式的数据集文件 本组采用python中的pandas库中的pd.readcsv()方法读取

- ```python
  def getOriData():
      #读取原始CSV数据 未经处理
      train = pd.read_csv('mnist_train.csv',header=None)  #没有表头 header指定为None
      test = pd.read_csv('mnist_test.csv',header = None)
      return train,test
  ```

- 读取后，查看概览信息如下：

- ![image-20221027165218674](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027165218674.png)

- ![image-20221027164735074](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027164735074.png)

- 训练集共有60000行数据，每一条数据行的第一列为标签(label)，即为对应的手写数字

- 其余784行为特征，因为是28*28=784的位图格式，共计784个特征

- 显而易见，需要对数据进行切片处理，将数据条中的特征列和标签列分开，分成label和feature

- ```python
  train.loc[i][1:]=train.loc[i][1:].apply(Binarization)
  #二维数组的切片不易理解 转换为一维切片更易理解
  test.loc[i][1:]=test.loc[i][1:].apply(Binarization)
  #拆分标签和特征向量
  label = trainData.iloc[:,0]
  feature = trainData.iloc[:,1:]
  return label,feature
  ```

#### 二值化映射

- 题目中指出，将数据特征进行二值化处理以方便分类，编写二值化映射函数如下：

- ```python
  #二值化映射函数
  def Binarization(val):
      tag = 255/2
      if val < tag:
          return 0
      else:
          return 1
  ```

- 数据的二值化映射一开始编写的是双重for循环，时空复杂度过高，查阅资料，发现可以使用pandas库中的apply函数，对dataframe中的每一个数据执行函数映射

- 完整的数据处理函数如下：

- ```python
  def DataProcess(train,test):
      for i in range(train.shape[0]):
          #二维数组的切片不易理解 转换为一维切片更易理解
          train.loc[i][1:]=train.loc[i][1:].apply(Binarization)
      for i in range(test.shape[0]):
          #二维数组的切片不易理解 转换为一维切片更易理解
          test.loc[i][1:]=test.loc[i][1:].apply(Binarization)
      #拆分标签和特征向量
      label = trainData.iloc[:,0]
      feature = trainData.iloc[:,1:]
      return label,feature
  ```



#### 独立二值特征朴素贝叶斯分类器

- 通过查阅互联网资料及书本P41页的独立二值特征朴素贝叶斯，原理概括如下：
- 独立二值特征：特征只有两种，此次实验中，图像的特征有且只有0，1两类 0表示黑色 1表示白色
- ![image-20221027165521352](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027165521352.png)
- 在实际过程中，分母有可能出现计数为0的情况，这时，会抛出除0异常
- 因此引入拉普拉斯平滑
- ![image-20221027165611127](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027165611127.png)

#### 分类器公式

![image-20221027165651088](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027165651088.png)

#### 计算分类器参数

- 计算先验概率（函数参数已给出完整注释）

- ```python
  def getPCi(data):
      #计算先验概率 P(C|J)
      tag=0
      counts = [0]*10
      for i in range(data.shape[0]):
          tag=data.loc[i][0]
          counts[tag]+=1
      PCi=np.zeros(10)  #先验概率
      for i in range(10):
          PCi[i] = counts[i]/60000
      return counts,PCi
  ```

- 分类器训练函数（函数参数已给出完整注释）

- ```python
  def TrainModel(df_feature,df_label,counts,k,n,labelNum):
      """
      Parameters:
      df_feature: 特征向量     类型:DataFrame  二值特征 0，1
      df_label: 标签向量       类型:DataFrame 其中值取0-9
      k:拉普拉斯平滑参数     类型:int
      counts:各个类别的样本数量 类型:array
      N:特征种数  本题为2
      labelNum:类别种数        类型:int
      """
      vecc = []   #存放的是每一个的P(Xj|Ci) 10*784
      for i in range(labelNum):
          print("当前正在训练的类别为:手写数字{}类".format(i))
          print("计算I(xj=1,c{})".format(i))
          I = feature[df_label==i].apply(np.sum,axis=0)  #获得每一个特征列中样本特征取值为1的个数
          p=(I+k) / (counts[i]+k*n)  #运用拉普拉斯平滑 计算P(Xj|Ci)
          vecc.append(p)
          print("I(xj=1,c{})={}".format(i,vecc[i][1]))
          print("手写数字{}类训练完成".format(i))
      return vecc
  ```

- 分类函数（函数参数已给出完整注释）

- ```python
  def ClassifySamples(sample,PCi,VEC_P):
      """
      Parameters:
      sample: 训练样本 10*784    类型:二维ndarray
      PCJ: 先验概率   10    类型:float64
      VEC_P: 即E矩阵
      """
      step1 = np.subtract(sample,1)  #数组数据相减
      step2 = np.abs(step1)  #求出绝对值
      step3 = np.subtract(step2,VEC_P)  #再相减
      step4=np.abs(step3) #求出最后的值
      log_P_Ci = np.log(PCi)
      log_P_Xj_Ci = np.log(step4)
      #计算P(Ci|x)=log(P(Ci))+Σlog(P(Xj|Ci))
      P_Ci_X = np.add(log_P_Ci,np.sum(log_P_Xj_Ci,axis=1))
      return np.argmax(P_Ci_X)
  ```

  

### （3）对10000张测试图片进行测试，并计算算法识别准确度

- 分类测试函数如下：

- ```python
  def ClassifyTestData(TestData,PCi,VEC_P):
      temp = []
      correctNum = 0
      FalseNum = 0
      for i in range(TestData.shape[0]):
          sample = TestData.iloc[i,1:].values
          MyAns = ClassifySamples(sample,PCi,VEC_P)
          Ans = TestData.loc[i][0]
          if i%5 == 0:
              print("Ans={},MyAns={}".format(Ans,MyAns))
          if MyAns == Ans:
              correctNum += 1
          else:
              FalseNum += 1
      print("CorrectNum={}".format(correctNum))
      print("FalseNum={}".format(FalseNum))
      correctRate = correctNum / (correctNum+FalseNum)
      return correctRate
  ```

  ![image-20221027172034467](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027172034467.png)

  ```python
  vec = TrainModel(feature,label,counts,3,2,10)
  ```

- 当拉普拉斯平滑参数k取3时：准确度为0.8419

- ![image-20221027172130534](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027172130534.png)

![image-20221027172146604](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027172146604.png)

### （4）算法分析

- 此次实验中在拉普拉斯平滑参数分别取0-5的情况下，平均出错数据约为1500条，准确度约为84%
- 经过统计发现，类似于4、7、9、8等图形较为复杂的数据出错频率较高
- 可能是由于这些图形图像较为复杂，特征值较为集中且易于混淆导致算法出错
- 分析算法的优缺点如下：
- MNIST数据集被预先处理过。通过上面的示例图片可以看出，MNIST中的图片较为纯净，没有噪声干扰，非常清晰。所以实验中可以直接进行二值化。
-  MNIST数据集中，图片中的数字总是在中心位置，大小合适，比较饱满。这一点保留了部分的空间信息。
- 误差可能的产生原因之一也有可能是在核心分类器函数中：
- ![image-20221027172625933](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027172625933.png)
- 许多接近于0的数字相乘，由于计算机精度的限制，导致大小比对时出现误差

## 实验分工

本小组于实验二发布当天即开始紧锣密鼓的学习工作，并于10月14日完成实验源代码撰写，10月27日完成实验报告的撰写，11月10日提交实验结果文档及源代码。

![image-20221110081843073](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221110081843073.png)

### 李欢欢

- 朴素贝叶斯理论学习
- 实验整体思路商讨、模块拆分、设计分类器
- 学习二值化朴素贝叶斯理论（伯努利形式）下的分类器设计与数学公式
- 查阅pandas文档，学习pandas中读取表格文件的方法
- 撰写getOriData()函数，实现了读取数据集，并分析数据集格式
- 处理数据，撰写了二值化映射函数
- 撰写计算先验概率模块，getPci()
- 撰写样本测试模块 **`ClassifySamples`**
- 分析比对不同拉普拉斯平滑参数下的分类器效率
- 撰写实验报告 小组讨论归纳整理总结

### 刘易行

- 朴素贝叶斯理论学习
- 实验整体思路商讨、模块拆分、设计分类器
- 翻阅资料，获得原始数据集（训练集、测试集）
- 学习二值化朴素贝叶斯理论（伯努利形式）下的分类器设计与数学公式
- 学习pandas中数组切片的奇技，当发现双循环切片时间复杂度过高时，选择使用pandas库中的dataframe.apply()方法，整体映射为二值化形式
- 撰写分类器训练模块，`TrainModel()`
- 分析比对不同拉普拉斯平滑参数下的分类器效率
- 撰写实验报告 小组讨论归纳整理总结

## 源代码

```python
import numpy as np    #计算数据
import matplotlib.pyplot as plt  #绘制图线
import pandas as pd  #读取csv文件 处理DataFrame

def getOriData():
    #读取原始CSV数据 未经处理
    train = pd.read_csv('mnist_train.csv',header=None)  #没有表头 header指定为None
    test = pd.read_csv('mnist_test.csv',header = None)
    return train,test

#二值化映射函数
def Binarization(val):
    tag = 255/2
    if val < tag:
        return 0
    else:
        return 1
    
def DataProcess(train,test):
    for i in range(train.shape[0]):
        #二维数组的切片不易理解 转换为一维切片更易理解
        train.loc[i][1:]=train.loc[i][1:].apply(Binarization)
    for i in range(test.shape[0]):
        #二维数组的切片不易理解 转换为一维切片更易理解
        test.loc[i][1:]=test.loc[i][1:].apply(Binarization)
    #拆分标签和特征向量
    label = trainData.iloc[:,0]
    feature = trainData.iloc[:,1:]
    return label,feature

def getPCi(data):
    #计算先验概率 P(C|J)
    tag=0
    counts = [0]*10
    for i in range(data.shape[0]):
        tag=data.loc[i][0]
        counts[tag]+=1
    PCi=np.zeros(10)  #先验概率
    for i in range(10):
        PCi[i] = counts[i]/60000
    return counts,PCi

def TrainModel(df_feature,df_label,counts,k,n,labelNum):
    """
    Parameters:
    df_feature: 特征向量     类型:DataFrame  二值特征 0，1
    df_label: 标签向量       类型:DataFrame 其中值取0-9
    k:拉普拉斯平滑参数     类型:int
    counts:各个类别的样本数量 类型:array
    N:特征种数  本题为2
    labelNum:类别种数        类型:int
    """
    vecc = []   #存放的是每一个的P(Xj|Ci) 10*784
    for i in range(labelNum):
        print("当前正在训练的类别为:手写数字{}类".format(i))
        print("计算I(xj=1,c{})".format(i))
        I = feature[df_label==i].apply(np.sum,axis=0)  #获得每一个特征列中样本特征取值为1的个数
        p=(I+k) / (counts[i]+k*n)  #运用拉普拉斯平滑 计算P(Xj|Ci)
        vecc.append(p)
        print("I(xj=1,c{})={}".format(i,vecc[i][1]))
        print("手写数字{}类训练完成".format(i))
    return vecc


def ClassifySamples(sample,PCi,VEC_P):
    """
    Parameters:
    sample: 训练样本 10*784    类型:二维ndarray
    PCJ: 先验概率   10    类型:float64
    VEC_P: 即E矩阵
    """
    step1 = np.subtract(sample,1)  #数组数据相减
    step2 = np.abs(step1)  #求出绝对值
    step3 = np.subtract(step2,VEC_P)  #再相减
    step4=np.abs(step3) #求出最后的值
    log_P_Ci = np.log(PCi)
    log_P_Xj_Ci = np.log(step4)
    #计算P(Ci|x)=log(P(Ci))+Σlog(P(Xj|Ci))
    P_Ci_X = np.add(log_P_Ci,np.sum(log_P_Xj_Ci,axis=1))
    return np.argmax(P_Ci_X)


def ClassifyTestData(TestData,PCi,VEC_P):
    temp = []
    correctNum = 0
    FalseNum = 0
    for i in range(TestData.shape[0]):
        sample = TestData.iloc[i,1:].values
        MyAns = ClassifySamples(sample,PCi,VEC_P)
        Ans = TestData.loc[i][0]
        if i%5 == 0:
            print("Ans={},MyAns={}".format(Ans,MyAns))
        if MyAns == Ans:
            correctNum += 1
        else:
            FalseNum += 1
    print("CorrectNum={}".format(correctNum))
    print("FalseNum={}".format(FalseNum))
    correctRate = correctNum / (correctNum+FalseNum)
    return correctRate

trainData,testData = getOriData()
counts,PCi=getPCi(trainData)
label,feature=DataProcess(trainData,testData)
vec = TrainModel(feature,label,counts,3,2,10)

#选取不同的拉普拉斯平滑参数进行测试
def RunNaiveBayes(k):
    trainData,testData = getOriData()
    counts,PCi=getPCi(trainData)
    label,feature=DataProcess(trainData,testData)
    vec = TrainModel(feature,label,counts,k,2,10)
    Rate=ClassifyTestData(testData,PCi,vec)
    return Rate

r1=RunNaiveBayes(1)
r2=RunNaiveBayes(2)
r3=RunNaiveBayes(3)
r4=RunNaiveBayes(4)
r5=RunNaiveBayes(5)

print("k=0,rate={}".format(r0))
print("k=1,rate={}".format(r1))
print("k=2,rate={}".format(r2))
print("k=3,rate={}".format(r3))
print("k=4,rate={}".format(r4))
print("k=5,rate={}".format(r5))
```

## **实验结果**

**分别设置不同的拉普拉斯平滑参数，测试贝叶斯分类器的效率性能**

![image-20221027173004867](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027173004867.png)

![image-20221027173014283](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027173014283.png)

![image-20221027173020347](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027173020347.png)

![image-20221027173026020](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027173026020.png)

![image-20221027173041297](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027173041297.png)

![image-20221027173047939](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20221027173047939.png)